{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM Analysis for Business Problem: Innactivity prediction with transactional data\n",
    "\n",
    "This notebook is a companion to the Medium article (link bellow) the underlies the application with CRISP-DM methodology to understand, analyze and communicate a business problem through a proven and tested Data Science methodology.\n",
    "\n",
    "CRISP-DM comprises of 6 steps:\n",
    "\n",
    "Section 1: Business Understanding\n",
    "\n",
    "Section 2: Data Understanding\n",
    "\n",
    "Section 3: Data Preparation\n",
    "\n",
    "Section 4: Data Modeling\n",
    "\n",
    "Section 5: Evaluate the Results\n",
    "\n",
    "Section 6: Deployment\n",
    "\n",
    "Medium Article:\n",
    "https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Business Understanding\n",
    "\n",
    "The broader business contextualization is laid in the companion Medium Article.\n",
    "A brief summary of the business undersating is laid out bellow:\n",
    "A Brazilian Credit Union wishes to preempively predict Mobile phone app innactivity in a six month window. \n",
    "\n",
    "The business value of such endeavor lies on: \n",
    "- (1) expanding use cases of a dataset (data enrichment may lead to revenue growth); \n",
    "- (2) deterring potential customer churn (avoid revenue lost);\n",
    "- (3) early detection of customer friction (garantee user satisfaction).\n",
    "\n",
    "The business questions that arise pertaing such objective are:\n",
    "\n",
    "### Question 1: Can transactional data alone safely predict channel innactivity in a six month window?\n",
    "\n",
    "### Question 2: What are the main features of a transactional dataset that can be used for understanding channel innactivity in a six month window?\n",
    "\n",
    "### Question 3: Are mono-product-family users more likely to have channel innactivity in a six month window?\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Understanding\n",
    "\n",
    "### Credit Union's Transaction Dataset overview\n",
    "\n",
    "Since this project deals with classified company owned information, we won't be able to show a complete exploratory analysis.\n",
    "\n",
    "So here is a small low level description:\n",
    "\n",
    "The Credit Union has several client channels. For this project we are looking at only one of them: the mobile phone app. It has roughlly 4 million users, with an average of 40–45 Million transactions per month, about 40% of these are financial transacions (like paying a bill) and 60% non-financial (like looking up a bill receipt). Our main goal for the project is avoidind financial transaction innactivity, so we focused onlty on those.\n",
    "\n",
    "All of these transactions are stored in a main database that is daily ingested in AWS Data Lake. That was the interface I used to query the data and extract it for the project.\n",
    "\n",
    "The transacional database holds A LOT of information. But, for this project the most vital informations used were:\n",
    "\n",
    "- Time and date the transacion happend;\n",
    "- The transaction code;\n",
    "- The product family the transaction is part of (example: investment application and investment cashout are two different transactions of the same product family).¹\n",
    "- The Credit Union Member who solicited the transaction;\n",
    "- The Credit Union the Member is linked to;\n",
    "- The status of the transacion. Did it complete? Or was it canceled?\n",
    "- The channel through which the transaction was solicited;\n",
    "\n",
    "¹ There are 8 main product familys: Channels (managing your self service channel), Checking account (wire transfers), Payments (Government Tribute or company Slips), Bills (Water, Phone, etc), Credit (Loans), Cards (Credit and debit) and PIX(Brazil’s own instant payment financial product), Investments (Long Term Deposits, Market Shares);\n",
    "\n",
    "For this project I filtered the channel to be only the Mobile App. I also chose 5 medium sized Credit Unions from our system (we have over 140) so as to have a good amount of data, but not too much as to make the processing time too long. And also fixed a six month period to analyze data.\n",
    "\n",
    "### IMPORTANT OBSERVATION: \n",
    "This dataset is quite clean because it’s a high management information system. When we use the filters described above, like the channel filter and completed status filter, we flush out basically anything that could get in our way. The heavier data wrangling necessary is grouping the transaction codes into product families and that is still quite easy to accomplish.\n",
    "\n",
    "### Exploratory Analysis of the Transactional Database\n",
    "\n",
    "I have written a second article piece that show cases the method I used for both the exploratory analysis and also the model selection and development. Please check it out the article, specially the <b>Data understanding — What data do we have / need? Is it clean?</b> section for further insight.\n",
    "\n",
    "Link:\n",
    "https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674\n",
    "\n",
    "### Disclaimer about Compliance and Confidentiality\n",
    "\n",
    "Due to company compliance I had to do all of the data wrangling and manipulation on our AWS Data Lake server using Redash running a AWS Athena and AthenaSQL engine. Data was only available for extraction after anonymization. I've included in the repository a SQL file with a pseudo algorhitm that masks the sensible information (like dataset names and columns) and shows how data manipulation was done.\n",
    "\n",
    "GitHub Repo for this project: https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data\n",
    "\n",
    "##### Queries using during exploratory analysis:\n",
    "1. pseudo query - exploratory analysis dataset (anonymous).sql :\n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20exploratory%20analysis%20dataset%20(anonymous).sql\n",
    "2. pseudo query - exporatory analysis - churn flags (anonymous).sql : \n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20exporatory%20analysis%20-%20churn%20flags%20(anonymous).sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Data Preparation\n",
    "\n",
    "### Classifier Models Data Set preparation\n",
    "\n",
    "There can be many approches when it comes to modelling this specific business problem. One way to look at is to think of this sixth month innactivity as a kind of “churn” that we would want to predict based on a series of features (predictors). On this approch we could elect a Classifier Model for the problem.\n",
    "\n",
    "On this solution framing we have to consider our dataset modeling base on individual and not on transactions (the would work for the Time Series model though).\n",
    "\n",
    "We need one individual per row, with all the features laid out on separate columns. Based on the exploratory analysis I want to construct my dataframe with the following blocks:\n",
    "\n",
    "- Account Number ID\n",
    "- Credit Union Number ID\n",
    "- Sixth Month Innactivity Flag (our future dependent variable)\n",
    "- A depth counter (number of transacionts) by month and by product family\n",
    "- An amplitude counter (number of diferente families used) by month\n",
    "- Total depth counter by month\n",
    "- Standard Deviation (by product family)\n",
    "- 3 month window Moving Average (by product family)\n",
    "- Absolute Moving Average variation (by product family)\n",
    "\n",
    "The first 6 columns were prepare via SQL query. The last three items were calculated in a second notebook also found in this Repository called 'Data Wrangling for Innactivity Prediction'.\n",
    "\n",
    "I extracted the data from the other 4 credit unions I had previously selected. This time bringing in every member who attendend one simple rule: they had to be active on the first by months of 2022. This extraction gave me a 91.848 long dataset, each row representing an unique individual.\n",
    "\n",
    "### Disclaimer about Compliance and Confidentiality\n",
    "\n",
    "Due to company compliance I had to do all of the data wrangling and manipulation on our AWS Data Lake server using Redash running a AWS Athena and AthenaSQL engine. Data was only available for extraction after anonymization. I've included in the repository a SQL file with a pseudo algorhitm that masks the sensible information (like dataset names and columns) and shows how data manipulation was done.\n",
    "\n",
    "GitHub Repo for this project: https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data\n",
    "\n",
    "##### Query used to generate model dataset\n",
    "1. pseudo query - model dataset.SQL : \n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20model%20dataset.SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Data Modeling\n",
    "\n",
    "The following section details the development of 10 different Classifiers Models aimed at supporting the analyses of the three business questions.\n",
    "\n",
    "I have written a second article piece that show cases the method I used for both the exploratory analysis and also the model selection and development. Please check it out the article, specially the <b>Modeling — What modeling techniques should we apply?</b> and the <b>Evaluation — Which model best meets the business objectives?</b> sections for further insight.\n",
    "\n",
    "Link: https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils class with functions for model development, testing and evaluation\n",
    "import utils as u\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CREDIT_UNION_ID</th>\n",
       "      <th>ACCOUNT_NUM</th>\n",
       "      <th>FLG_202201</th>\n",
       "      <th>FLG_202202</th>\n",
       "      <th>FLG_202203</th>\n",
       "      <th>FLG_202204</th>\n",
       "      <th>FLG_202205</th>\n",
       "      <th>FLG_202206</th>\n",
       "      <th>DEEP_CHANNELS_202201</th>\n",
       "      <th>...</th>\n",
       "      <th>PAYMENTS_MA_3</th>\n",
       "      <th>PAYMENTS_MA_VAR</th>\n",
       "      <th>AMP_MA_1</th>\n",
       "      <th>AMP_MA_2</th>\n",
       "      <th>AMP_MA_3</th>\n",
       "      <th>AMP_MA_VAR</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_1</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_2</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_3</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_VAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>ZWZZ!W</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>94.666667</td>\n",
       "      <td>105.333333</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>&amp;WXYY&amp;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>Y%@YZ&amp;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>!W%&amp;#!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.666667</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>21.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>%##AXY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>-7.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 CREDIT_UNION_ID ACCOUNT_NUM  FLG_202201  FLG_202202  FLG_202203  \\\n",
       "0           0               A      ZWZZ!W           1           1           1   \n",
       "1           1               A      &WXYY&           1           1           1   \n",
       "2           2               A      Y%@YZ&           1           1           1   \n",
       "3           3               A      !W%&#!           1           1           1   \n",
       "4           4               A      %##AXY           1           1           1   \n",
       "\n",
       "   FLG_202204  FLG_202205  FLG_202206  DEEP_CHANNELS_202201  ...  \\\n",
       "0           1           1           1                     0  ...   \n",
       "1           1           1           1                     0  ...   \n",
       "2           1           1           1                     0  ...   \n",
       "3           1           1           1                     0  ...   \n",
       "4           1           1           1                     0  ...   \n",
       "\n",
       "   PAYMENTS_MA_3  PAYMENTS_MA_VAR  AMP_MA_1  AMP_MA_2  AMP_MA_3  AMP_MA_VAR  \\\n",
       "0       8.666667         2.666667  3.666667  3.333333  2.666667   -1.000000   \n",
       "1       0.000000        -2.000000  2.000000  1.666667  1.666667   -0.333333   \n",
       "2       0.000000         0.000000  2.000000  2.000000  1.666667   -0.333333   \n",
       "3       1.333333         0.000000  2.666667  3.000000  2.666667    0.000000   \n",
       "4       2.666667        -0.666667  3.000000  2.666667  2.666667   -0.333333   \n",
       "\n",
       "   NUM_TRANSACTIONS_MA_1  NUM_TRANSACTIONS_MA_2  NUM_TRANSACTIONS_MA_3  \\\n",
       "0              94.666667             105.333333              99.333333   \n",
       "1               8.666667              12.000000              13.333333   \n",
       "2               9.333333               9.333333              11.333333   \n",
       "3              32.666667              38.000000              54.000000   \n",
       "4              16.000000              10.666667               8.666667   \n",
       "\n",
       "   NUM_TRANSACTIONS_MA_VAR  \n",
       "0                 4.666667  \n",
       "1                 4.666667  \n",
       "2                 2.000000  \n",
       "3                21.333333  \n",
       "4                -7.333333  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first_run = u.pd.read_csv('./Model Data Set STDEV MA (pseudo).csv',sep=',')\n",
    "df_first_run.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FLG_202201</th>\n",
       "      <th>FLG_202202</th>\n",
       "      <th>FLG_202203</th>\n",
       "      <th>FLG_202204</th>\n",
       "      <th>FLG_202205</th>\n",
       "      <th>FLG_202206</th>\n",
       "      <th>DEEP_CHANNELS_202201</th>\n",
       "      <th>DEEP_CHANNELS_202202</th>\n",
       "      <th>DEEP_CHANNELS_202203</th>\n",
       "      <th>...</th>\n",
       "      <th>PAYMENTS_MA_3</th>\n",
       "      <th>PAYMENTS_MA_VAR</th>\n",
       "      <th>AMP_MA_1</th>\n",
       "      <th>AMP_MA_2</th>\n",
       "      <th>AMP_MA_3</th>\n",
       "      <th>AMP_MA_VAR</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_1</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_2</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_3</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_VAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>79.333333</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>-14.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>38.666667</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  FLG_202201  FLG_202202  FLG_202203  FLG_202204  FLG_202205  \\\n",
       "0           0           1           1           1           1           1   \n",
       "1           1           1           1           1           1           1   \n",
       "2           2           1           1           1           1           1   \n",
       "3           3           1           1           1           1           1   \n",
       "4           4           1           1           1           1           1   \n",
       "\n",
       "   FLG_202206  DEEP_CHANNELS_202201  DEEP_CHANNELS_202202  \\\n",
       "0           1                     0                     0   \n",
       "1           1                     0                     0   \n",
       "2           1                     0                     0   \n",
       "3           1                     0                     0   \n",
       "4           1                     0                     0   \n",
       "\n",
       "   DEEP_CHANNELS_202203  ...  PAYMENTS_MA_3  PAYMENTS_MA_VAR  AMP_MA_1  \\\n",
       "0                     0  ...      10.666667         2.666667  2.333333   \n",
       "1                     0  ...       3.333333         2.000000  2.000000   \n",
       "2                     0  ...      15.333333         5.333333  2.333333   \n",
       "3                     0  ...       1.333333         1.333333  2.000000   \n",
       "4                     0  ...       4.000000         1.333333  3.000000   \n",
       "\n",
       "   AMP_MA_2  AMP_MA_3  AMP_MA_VAR  NUM_TRANSACTIONS_MA_1  \\\n",
       "0  2.666667  3.000000    0.666667              26.666667   \n",
       "1  2.000000  2.000000    0.000000              18.000000   \n",
       "2  2.666667  2.666667    0.333333              53.333333   \n",
       "3  2.000000  1.666667   -0.333333              34.000000   \n",
       "4  3.000000  3.333333    0.333333              25.333333   \n",
       "\n",
       "   NUM_TRANSACTIONS_MA_2  NUM_TRANSACTIONS_MA_3  NUM_TRANSACTIONS_MA_VAR  \n",
       "0              29.333333              32.000000                 5.333333  \n",
       "1              18.666667              19.333333                 1.333333  \n",
       "2              72.000000              79.333333                26.000000  \n",
       "3              20.666667              19.333333               -14.666667  \n",
       "4              29.333333              38.666667                13.333333  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big_blind_test = u.pd.read_csv('./Big Blind Predict Test.csv',sep=',')\n",
    "df_big_blind_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make sure to create a copy of the data before we start altering it. Note that we don't change the original data we loaded.\n",
    "data_first_run = df_first_run.copy(deep=False)\n",
    "data_big_blind_test = df_big_blind_test.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLG_INNACTIVITY</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_1</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_2</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_3</th>\n",
       "      <th>NUM_TRANSACTIONS_MA_VAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94.666667</td>\n",
       "      <td>105.333333</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32.666667</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>21.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>-7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91843</th>\n",
       "      <td>1</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>79.333333</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91844</th>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91845</th>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91846</th>\n",
       "      <td>1</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>-6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91847</th>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91848 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FLG_INNACTIVITY  NUM_TRANSACTIONS_MA_1  NUM_TRANSACTIONS_MA_2  \\\n",
       "0                    1              94.666667             105.333333   \n",
       "1                    1               8.666667              12.000000   \n",
       "2                    1               9.333333               9.333333   \n",
       "3                    1              32.666667              38.000000   \n",
       "4                    1              16.000000              10.666667   \n",
       "...                ...                    ...                    ...   \n",
       "91843                1              76.666667              79.333333   \n",
       "91844                1              16.000000              18.666667   \n",
       "91845                1               3.000000               2.666667   \n",
       "91846                1              15.333333               8.000000   \n",
       "91847                1               2.000000               3.333333   \n",
       "\n",
       "       NUM_TRANSACTIONS_MA_3  NUM_TRANSACTIONS_MA_VAR  \n",
       "0                  99.333333                 4.666667  \n",
       "1                  13.333333                 4.666667  \n",
       "2                  11.333333                 2.000000  \n",
       "3                  54.000000                21.333333  \n",
       "4                   8.666667                -7.333333  \n",
       "...                      ...                      ...  \n",
       "91843             102.666667                26.000000  \n",
       "91844              18.000000                 2.000000  \n",
       "91845               2.666667                -0.333333  \n",
       "91846               8.666667                -6.666667  \n",
       "91847               4.000000                 2.000000  \n",
       "\n",
       "[91848 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_first_run = u.pd.DataFrame()\n",
    "data_first_run['FLG_INNACTIVITY'] = df_first_run['FLG_202206']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_1'] = df_first_run['NUM_TRANSACTIONS_MA_1']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_2'] = df_first_run['NUM_TRANSACTIONS_MA_2']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_3'] = df_first_run['NUM_TRANSACTIONS_MA_3']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_VAR'] = df_first_run['NUM_TRANSACTIONS_MA_VAR']\n",
    "data_first_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_first_run = u.pd.DataFrame()\n",
    "data_first_run['FLG_INNACTIVITY'] = df_first_run['FLG_202206']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_1'] = df_first_run['NUM_TRANSACTIONS_MA_1']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_2'] = df_first_run['NUM_TRANSACTIONS_MA_2']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_3'] = df_first_run['NUM_TRANSACTIONS_MA_3']\n",
    "data_first_run['NUM_TRANSACTIONS_MA_VAR'] = df_first_run['NUM_TRANSACTIONS_MA_VAR']\n",
    "data_first_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling class imbalance\n",
    "\n",
    "We know from our exploratory analysis that this dataset will be havily imbalanced with churn on 6th month as the minority class (represented as inactivity on that month or FLG_202206 = 0). Check the histogram bellow for visual reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = u.plt.subplots()\n",
    "axis.hist(data_first_run['FLG_INNACTIVITY'])\n",
    "u.plt.ylabel('Accounts')\n",
    "u.plt.xlabel('Innactivity Flag')\n",
    "u.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with classifiers and class imbalance is that the classifier will more easily classify the majority class, simply because most cases are of that class. For that reason model performance metrics have to be carefully selected. Precision, recall and F1 will be used as the main metrics for evaluating performance. In our specfic case we our most interested in those metrics regarding the prediction of the minority class (0 in our case).\n",
    "\n",
    "So in this study we will contrast the use of two wildly used classification models: Logistic Regression and RandomTreeClassifier, both with SciKit Learn implementations. Tree Ensembles our suposabily better at handling inbalance. And a common technique for getting better results is using resampling techniques. For that we will contrast model metrics on baseline models with resampled models (RandomOverSampling, SMOTE and NearMisses)\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://medium.com/grabngoinfo/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037\n",
    "\n",
    "https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare independent variables (X) and dependent variable (y) for the Model Training and Setup dataset\n",
    "X = data_first_run.loc[:,'DEEP_CHANNELS_202201':'NUM_TRANSACTIONS_MA_VAR'] # Drop Id columns\n",
    "X = X.drop(columns=['FLG_INNACTIVITY',\n",
    "                    'DEEP_CHANNELS_202206',\n",
    "                    'DEEP_CARDS_202206',\n",
    "                    'DEEP_CHECKING_202206',\n",
    "                    'DEEP_BILLS_202206',\n",
    "                    'DEEP_CREDIT_202206',\n",
    "                    'DEEP_INVESTMENTS_202206',\n",
    "                    'DEEP_PAYMENTS_202206',\n",
    "                    'DEEP_PIX_202206',\n",
    "                    'AMP_202206',\n",
    "                    'NUM_TRANSACTIONS_202206'\n",
    "                   ]) # Drop prediction column\n",
    "y = data_first_run['FLG_INNACTIVITY']\n",
    "\n",
    "#Declare independent variables (X) and dependent variable (y) for the Big Blind Predict Test dataset\n",
    "X_bbt = data_big_blind_test.loc[:,'DEEP_CHANNELS_202201':'NUM_TRANSACTIONS_MA_VAR'] # Drop Id columns\n",
    "X_bbt = X_bbt.drop(columns=['FLG_INNACTIVITY',\n",
    "                    'DEEP_CHANNELS_202206',\n",
    "                    'DEEP_CARDS_202206',\n",
    "                    'DEEP_CHECKING_202206',\n",
    "                    'DEEP_BILLS_202206',\n",
    "                    'DEEP_CREDIT_202206',\n",
    "                    'DEEP_INVESTMENTS_202206',\n",
    "                    'DEEP_PAYMENTS_202206',\n",
    "                    'DEEP_PIX_202206',\n",
    "                    'AMP_202206',\n",
    "                    'NUM_TRANSACTIONS_202206'\n",
    "                   ]) # Drop prediction column\n",
    "y_bbt = data_big_blind_test['FLG_INNACTIVITY']\n",
    "\n",
    "#set shared model, scaler and splitter variables\n",
    "random_state = 42\n",
    "test_size = 0.30\n",
    "verbose = 'off'\n",
    "print_report = 'off'\n",
    "\n",
    "#set model names\n",
    "models = [\n",
    "          'Random Forest',\n",
    "#           'Logistic Regression',\n",
    "         ]\n",
    "\n",
    "#set resampling method names\n",
    "resamplers = [\n",
    "              'Baseline',\n",
    "              'Random Over Sampling',\n",
    "              'SMOTE',\n",
    "              'Near Miss KNN',\n",
    "              'Random Under Sampling',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u = u.reload(u)\n",
    "model_prediction, model_scores_table, model_coef_table, BBT_model_scores_table = u.model_run(models,\n",
    "                                                                                           resamplers,\n",
    "                                                                                           X,\n",
    "                                                                                           y,\n",
    "                                                                                           X_bbt,\n",
    "                                                                                           y_bbt,\n",
    "                                                                                           random_state,\n",
    "                                                                                           test_size,\n",
    "                                                                                           verbose,\n",
    "                                                                                           print_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training Score Table\n",
    "m = model_scores_table.loc[model_scores_table['Precision 1'] >= .08].sort_values(by='Recall 1')\n",
    "#m = m.loc[m['Recall 1'] >= .85]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training coeficient and feature importance table\n",
    "model_coef_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blind Test Score Table\n",
    "BBT_model_scores_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Evaluate the Results\n",
    "\n",
    "This section will split up into separate analyses for each business question.\n",
    "Each section will be comprised of a brief analysis, and evaluation and conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Can transactional data alone safely predict channel innactivity in a six month window?\n",
    "\n",
    "In this section we will analyze the Model Scores table for the Big Blind Test routine. The idea is to simulate a real world application where we will use a data sample to train a model to predict in a bigger group.\n",
    "\n",
    "Let's first analyze the scores to identify our best models.\n",
    "\n",
    "Our Models can be considerd having medium to high success if they:\n",
    "1) Correctly predict over 80% of the Tre Positive Cases\n",
    "We don't want a model who can get most our cases right from the start. In this case, our main interest is the minority case (1).\n",
    "\n",
    "2) Correctly predict over 95% of the Tre Negative Cases\n",
    "The should also be able to get most of the True Negatives cases (minority, 0) since it will have abundant data for the job.\n",
    "\n",
    "2) Have a Precision higher than 75% for the minority class\n",
    "The rate in which the model classifies True Positives is higher than that of False Positives.\n",
    "\n",
    "3) Have a Recall higher than 80% \n",
    "The rate in which the model correctly clasifies all possible class cases, in this case the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Primer com classifier evaluation scores\n",
    "\n",
    "Quick Primer on reading the Confusion Matrix and Classification report measures\n",
    "How to read the quadrants of the matrix:\n",
    "\n",
    "True Negative | False Positive\n",
    "\n",
    "False Negative | True Positive\n",
    "\n",
    "Precision\n",
    "Measure of how many of the positive predictions made are correct (true positives).\n",
    "Formula: TP/(TP+FP)\n",
    "\n",
    "Recall\n",
    "Measure of how many of the positive cases the classifier correctly predicted considering the over all positive cases in the data.\n",
    "It is sometimes also referred to as Sensitivity\n",
    "Formula: TP/(TP+FN)\n",
    "\n",
    "f1-Score\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "Accuracy\n",
    "Measure of the number of correct predictions over all predictions\n",
    "Formula: (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBT_model_scores_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cutoff = BBT_model_scores_table\n",
    "i=0.1\n",
    "for count in range(10):\n",
    "    model_cutoff = BBT_model_scores_table.loc[BBT_model_scores_table['TP'] >= (Support_1*i)]\n",
    "    print('Cutoff:',i,'Number of models:',model_cutoff.shape[0])\n",
    "    i+=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cutoff = BBT_model_scores_table\n",
    "i=0.05\n",
    "for count in range(20):\n",
    "    model_cutoff = BBT_model_scores_table.loc[BBT_model_scores_table['TP'] >= (Support_1*.7)]\n",
    "    model_cutoff = model_cutoff.loc[BBT_model_scores_table['TN'] >= (Support_0*.7)]\n",
    "    model_cutoff = model_cutoff.loc[BBT_model_scores_table['Precision 1'] >= i]\n",
    "    print('Cutoff:',i,'Number of models:',model_cutoff.shape[0])\n",
    "    i+=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Support_1 = BBT_model_scores_table.loc[1,'Support 1']\n",
    "Support_0 = BBT_model_scores_table.loc[1,'Support 0']\n",
    "\n",
    "# 1) Correctly predict over 80% of the Tre Positive Cases \n",
    "model_cutoff = BBT_model_scores_table.loc[BBT_model_scores_table['TP'] >= (Support_1*.7)]\n",
    "print('Total number of models after cutoff round:',model_cutoff.shape[0])\n",
    "\n",
    "# 2) Correctly predict over 95% of the Tre Negative Cases\n",
    "model_cutoff = model_cutoff.loc[BBT_model_scores_table['TN'] >= (Support_0*.7)]\n",
    "print('Total number of models after cutoff round:',model_cutoff.shape[0])\n",
    "\n",
    "# # 3) Have a Precision higher than 75% for the minority class\n",
    "# model_cutoff = model_cutoff.loc[BBT_model_scores_table['Precision 1'] >= 0.75]\n",
    "# print('Total number of models after cutoff round:',model_cutoff.shape[0])\n",
    "\n",
    "# # 4) Have a Recall higher than 80%\n",
    "# model_cutoff = model_cutoff.loc[BBT_model_scores_table['Recall 1'] >= 0.85]\n",
    "# print('Total number of models after cutoff round:',model_cutoff.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our Top 3 Models, sorted out by Precision 1 value\n",
    "model_cutoff.sort_values(by='Precision 1',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfication Score analysis\n",
    "\n",
    "Our Top 3 Models are all Random Forest, but with Resampling variations: Random Over Sampling, Baseline (no resampling) and Near Miss KNN.\n",
    "\n",
    "All three had very similiar performances. All three had 100% Recall, which means they correctly predicted all cases of True Positives available in the data. All three also had 100% Precision on the major class (0), which means they got all True Negatives right and had zero False Negatives to account for.\n",
    "\n",
    "The main point which differentiates the three models are the False Negative scores and, consequently, the Precision 1 scores. Random Forest with Random Over Sampling outperfomed both the Baseline and the Neas Miss KNN variates by predicting less False Positives, which in turn led to a Precision 1 score slightly above the other two models.\n",
    "\n",
    "## Evaluation for Question 1:\n",
    "\n",
    "We have three models who achieve (and in fact surpass) our success pre-requisites. Thus, we can than affirm that the Classifier Models will safely predict innactivity on month six."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: What are the main features of a transactional dataset that can be used for understanding channel innactivity in a six month window?\n",
    "\n",
    "The objetctive behind this question is to understand what predictors from our transactional database have the higest impact on model performance.\n",
    "\n",
    "We will:\n",
    "\n",
    "(1) Use the model coeficient and features importance to select the Top 10 features\n",
    "\n",
    "(3) Reavaluate our models using only the best predictor to check if performance boosts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a Table of only the Top 3 Model's Feature Importance Scores\n",
    "Top3_FI = model_coef_table.loc[:,['Features','Random Forest Baseline','Random Forest Random Over Sampling','Random Forest Near Miss KNN']]\n",
    "Top3_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many features were fed to the model\n",
    "print('Total number of features fed to the model = ',Top3_FI.shape[0])\n",
    "\n",
    "# Lets Sum and Average the Importance Feature Scores\n",
    "Top3_FI['Features Importance Average'] = Top3_FI.drop(columns=['Features']).mean(axis = 1)\n",
    "\n",
    "#Let's Check how many Features show an average importance of 0.\n",
    "print('Average Feature Importance of zero =',Top3_FI.loc[Top3_FI['Features Importance Average'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's analyze a histogram of Coeficient and Feature Importance scores.\n",
    "u.pyplot.hist(Top3_FI['Features Importance Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the coeficient score\n",
    "print(Top3_FI['Features Importance Average'].describe(include='all'))\n",
    "print('Average Feature Importance >= 0.01 =',Top3_FI.loc[Top3_FI['Features Importance Average'] >= 0.01].shape[0])\n",
    "print('Average Feature Importance >= 0.02 =',Top3_FI.loc[Top3_FI['Features Importance Average'] >= 0.02].shape[0])\n",
    "print('Average Feature Importance >= 0.03 =',Top3_FI.loc[Top3_FI['Features Importance Average'] >= 0.03].shape[0])\n",
    "print('Average Feature Importance >= 0.04 =',Top3_FI.loc[Top3_FI['Features Importance Average'] >= 0.04].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop the Features of average importance < 0.01\n",
    "Top3_FI_Clean = Top3_FI.loc[Top3_FI['Features Importance Average'] >= 0.01]\n",
    "print('Total number of features with imporantce > 0 =',Top3_FI_Clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we've cleand things up, let's analyze again a histogram of Coeficient and Feature Importance scores.\n",
    "u.pyplot.hist(Top3_FI_Clean['Features Importance Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the Top 10 coeficient for the Random ForestRandom Under Sampling?\n",
    "Top3_FI_Clean.sort_values(by=['Features Importance Average'],ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficient and Features Importance Analysis\n",
    "\n",
    "In this section we used Coeficiente and Feature Importance Analysis to identify the most important features on our Top 3 models.\n",
    "\n",
    "On the first few rounds of investigation we noticed a huge amount of features with coeficients scoring less that 0.01 importance. After cleaning them up we found 16 features with the highest importance. We will now test these on a new training and evaltuation routine to see if our model performance boosts up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tweaking based on feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare independent variables (X) and dependent variable (y)\n",
    "\n",
    "# Based on our prior analysis, I've decided to test the perfomance of the models with less predictors. \n",
    "# So will drop most of them and leave only the top 10.\n",
    "\n",
    "# To avoid writing them out every time, we save the names of the estimators of our model in a list. \n",
    "#Declare independent variables (X) and dependent variable (y) for the Model Training and Setup dataset\n",
    "X1 = data_first_run.loc[:,Top3_FI_Clean['Features']] # Drop Id columns\n",
    "y1 = data_first_run['FLG_INNACTIVITY']\n",
    "\n",
    "#Declare independent variables (X) and dependent variable (y) for the Big Blind Predict Test dataset\n",
    "X1_bbt = data_big_blind_test.loc[:,Top3_FI_Clean['Features']] # Drop Id columns\n",
    "y1_bbt = data_big_blind_test['FLG_INNACTIVITY']\n",
    "\n",
    "models1 = ['Random Forest']\n",
    "resamplers1 = ['Random Over Sampling',\n",
    "               'Baseline',\n",
    "               'Near Miss KNN'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.reload(u)\n",
    "model_prediction1, model_scores_table1, model_coef_table1, BBT_model_scores_table1 = u.model_run(models1,\n",
    "                                                                                           resamplers1,\n",
    "                                                                                           X1,\n",
    "                                                                                           y1,\n",
    "                                                                                           X1_bbt,\n",
    "                                                                                           y1_bbt,\n",
    "                                                                                           random_state,\n",
    "                                                                                           test_size,\n",
    "                                                                                           verbose,\n",
    "                                                                                           print_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBT_model_scores_table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Are mono-product-family users more likely to have channel innactivity in a six month window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Though the exploratory analysis indicated the possibily of finding correlation between transaction patterns and innactivity, the two classifiers and 4 resampling techniques used did not present good performance on this highly imbalanced dataset. \n",
    "\n",
    "The models just didn't perform well! Unfortunatelly. But hey, this is a scientific approach, know that something doesn't work is also a valid result, it just brushes off the false positives from your line of sight.\n",
    "\n",
    "All models had precision scores ranging from 0.08 to 0.09, recall from 0.83 to 0.85 and f1-score at exactlly 0.15. The main difference seeable at the confusion matrix, with slight differences on the true/false positive/negative predictions. The RandomForest with Random Under Sampling had similiar measures: precision at 0.09, recall at 0.77 and f1-score at 0.16.\n",
    "Exemple of Classification Report and Confusion Matrix for the Logistic Regression with Baseline model.\n",
    "\n",
    "The models actually did an interesting job of predicting 325+ cases of the 423 innactivity targets in the test set (you can see that looking at the confusion matrix's top left quadrant, 358 in the example above). That is why the Recall (or sensitivity) is high. \n",
    "\n",
    "This means the model is more confident at trying to predict the minority cases (the Random Forest Baseline practically didn't even try to predict the minority cases, in the report in only classified 15 as negatives, and 14 of them were flase - check the print screen bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Unfortunately this project doesn't seem to provide strong evidence towards answering either positively or negatively the business question provided.\n",
    "\n",
    "Our exploratory analysis show their is a potential correlation to be explored between innactivaty, depth (specially PIX) and amplitude. But, the use of classifier models, at least with the present configuration, haven't presented promising results.\n",
    "\n",
    "### Recommendations on future studies\n",
    "\n",
    "1. Study the use of time series prediction techniques as a subsititue for Classifiers\n",
    "2. Use the accumlative transactional variation on 5 months prior to the 6th month innactivity prediction may wielf better results than using the absolute number of transations per month as features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
