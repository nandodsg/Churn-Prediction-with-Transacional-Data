{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbb2247",
   "metadata": {},
   "source": [
    "# CRISP-DM Analysis for Business Problem: Innactivity prediction with transactional data\n",
    "\n",
    "This notebook is a companion to the Medium article (link bellow) the underlies the application with CRISP-DM methodology to understand, analyze and communicate a business problem through a proven and tested Data Science methodology.\n",
    "\n",
    "CRISP-DM comprises of 6 steps:\n",
    "\n",
    "Section 1: Business Understanding\n",
    "\n",
    "Section 2: Data Understanding\n",
    "\n",
    "Section 3: Data Preparation\n",
    "\n",
    "Section 4: Data Modeling\n",
    "\n",
    "Section 5: Evaluate the Results\n",
    "\n",
    "Section 6: Deployment\n",
    "\n",
    "Medium Article:\n",
    "https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c008d8",
   "metadata": {},
   "source": [
    "# Section 1: Business Understanding\n",
    "\n",
    "The broader business contextualization is laid in the companion Medium Article.\n",
    "A brief summary of the business undersating is laid out bellow:\n",
    "A Brazilian Credit Union wishes to preempively predict Mobile phone app innactivity in a six month window. \n",
    "\n",
    "The business value of such endeavor lies on: \n",
    "- (1) expanding use cases of a dataset (data enrichment may lead to revenue growth); \n",
    "- (2) deterring potential customer churn (avoid revenue lost);\n",
    "- (3) early detection of customer friction (garantee user satisfaction).\n",
    "\n",
    "The business questions that arise pertaing such objective are:\n",
    "\n",
    "### Question 1: What are aspects of a transactional dataset that can be used for understanding channel innactivity in a six month window?\n",
    "\n",
    "### Question 2: Are mono-product-family users more likely to have channel innactivity in a six month window?\n",
    "\n",
    "### Question 3: Can transactional data alone safely predict channel innactivity in a six month window?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68653ae5",
   "metadata": {},
   "source": [
    "# Section 2: Data Understanding\n",
    "\n",
    "### Credit Union's Transaction Dataset overview\n",
    "\n",
    "The Credit Union has several client channels. For this project we are looking at only of them: the mobile phone app. It has roughlly 4 million users, with an average of 40–45 Million transactions per month, about 40% of these are financial transacions (like paying a bill) and 60% non-financial (like looking up a bill receipt). Our main goal for the project is avoidind financial transaction innactivity, so we focused onlty on those.\n",
    "\n",
    "All of these transactions are stored in a main database that is daily ingested in AWS Data Lake. That was the interface I used to query the data and extract it for the project.\n",
    "\n",
    "The transacional database holds A LOT of information. But, for this project the most vital informations used were:\n",
    "\n",
    "- Time and date the transacion happend;\n",
    "- The transaction code;\n",
    "- The product family the transaction is part of (example: investment application and investment cashout are two different transactions of the same product family).¹\n",
    "- The Credit Union Member who solicited the transaction;\n",
    "- The Credit Union the Member is linked to;\n",
    "- The status of the transacion. Did it complete? Or was it canceled?\n",
    "- The channel through which the transaction was solicited;\n",
    "\n",
    "¹ There are 8 main product familys: Channels (managing your self service channel), Checking account (wire transfers), Payments (Government Tribute or company Slips), Bills (Water, Phone, etc), Credit (Loans), Cards (Credit and debit) and PIX(Brazil’s own instant payment financial product), Investments (Long Term Deposits, Market Shares);\n",
    "\n",
    "For this project I filtered the channel to be only the Mobile App. I also chose 5 medium sized Credit Unions from our system (we have over 140) so as to have a good amount of data, but not too much as to make the processing time too long. And also fixed a six month period to analyze data.\n",
    "\n",
    "### IMPORTANT OBSERVATION: \n",
    "This dataset is quite clean because it’s a high management information system. When we use the filters described above, like the channel filter and completed status filter, we flush out basically anything that could get in our way. The heavier data wrangling necessary is grouping the transaction codes into product families and that is still quite easy to accomplish.\n",
    "\n",
    "### Exploratory Analysis of the Transactional Database\n",
    "\n",
    "I have written a second article piece that show cases the method I used for both the exploratory analysis and also the model selection and development. Please check it out the article, specially the <b>Data understanding — What data do we have / need? Is it clean?</b> section for further insight.\n",
    "\n",
    "Link:\n",
    "https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674\n",
    "\n",
    "### Disclaimer about Compliance and Confidentiality\n",
    "\n",
    "Due to company compliance I had to do all of the data wrangling and manipulation on our AWS Data Lake server using Redash running a AWS Athena and AthenaSQL engine. Data was only available for extraction after anonymization. I've included in the repository a SQL file with a pseudo algorhitm that masks the sensible information (like dataset names and columns) and shows how data manipulation was done.\n",
    "\n",
    "GitHub Repo for this project: https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data\n",
    "\n",
    "##### Queries using during exploratory analysis:\n",
    "1. pseudo query - exploratory analysis dataset (anonymous).sql :\n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20exploratory%20analysis%20dataset%20(anonymous).sql\n",
    "2. pseudo query - exporatory analysis - churn flags (anonymous).sql : \n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20exporatory%20analysis%20-%20churn%20flags%20(anonymous).sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f241ffa",
   "metadata": {},
   "source": [
    "# Section 3: Data Preparation\n",
    "\n",
    "There can be many approches when it comes to modelling this specific business problem. One way to look at is to think of this sixth month innactivity as a kind of “churn” that we would want to predict based on a series of features (predictors). On this approch we could elect a Classifier Model for the problem.\n",
    "\n",
    "On this solution framing we have to consider our dataset modeling base on individual and not on transactions (the would work for the Time Series model though).\n",
    "\n",
    "We need one individual per row, with all the features laid out on separate columns. Based on the exploratory analysis I want to construct my dataframe with the following blocks:\n",
    "\n",
    "- Account Number ID\n",
    "- Credit Union Number ID\n",
    "- Sixth Month Innactivity Flag (our future dependent variable)\n",
    "- A depth counter (number of transacionts) by month and by product family\n",
    "- An amplitude counter (number of diferente families used) by month\n",
    "- Total depth counter by month\n",
    "\n",
    "I extracted the data from the other 4 credit unions I had previously selected. This time bringing in every member who attendend one simple rule: they had to be active on the first by months of 2022. This extraction gave me a 91.848 long dataset, each row representing an unique individual.\n",
    "\n",
    "### Disclaimer about Compliance and Confidentiality\n",
    "\n",
    "Due to company compliance I had to do all of the data wrangling and manipulation on our AWS Data Lake server using Redash running a AWS Athena and AthenaSQL engine. Data was only available for extraction after anonymization. I've included in the repository a SQL file with a pseudo algorhitm that masks the sensible information (like dataset names and columns) and shows how data manipulation was done.\n",
    "\n",
    "GitHub Repo for this project: https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data\n",
    "\n",
    "##### Query used to generate model dataset\n",
    "1. pseudo query - model dataset.SQL : \n",
    "https://github.com/nandodsg/Innactivity-Prediction-with-Transactional-Data/blob/main/pseudo%20query%20-%20model%20dataset.SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85910b08",
   "metadata": {},
   "source": [
    "# Section 4: Data Modeling\n",
    "\n",
    "The following section details the development of 10 different Classifiers Models aimed at supporting the analyses of the three business questions.\n",
    "\n",
    "I have written a second article piece that show cases the method I used for both the exploratory analysis and also the model selection and development. Please check it out the article, specially the <b>Modeling — What modeling techniques should we apply?</b> and the <b>Evaluation — Which model best meets the business objectives?</b> sections for further insight.\n",
    "\n",
    "Link: https://medium.com/@fernandocarliniguimaraes/innactivity-prediction-using-machine-learning-on-transacional-data-642ef7c84674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2fe729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils class with functions for model development, testing and evaluation\n",
    "import utils as u\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e30487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CREDIT_UNION_ID</th>\n",
       "      <th>ACCOUNT_NUM</th>\n",
       "      <th>FLG_202201</th>\n",
       "      <th>FLG_202202</th>\n",
       "      <th>FLG_202203</th>\n",
       "      <th>FLG_202204</th>\n",
       "      <th>FLG_202205</th>\n",
       "      <th>FLG_202206</th>\n",
       "      <th>DEEP_CHANNELS_202201</th>\n",
       "      <th>DEEP_CHANNELS_202202</th>\n",
       "      <th>...</th>\n",
       "      <th>AMP_202203</th>\n",
       "      <th>AMP_202204</th>\n",
       "      <th>AMP_202205</th>\n",
       "      <th>AMP_202206</th>\n",
       "      <th>NUM_TRANSACTIONS_202201</th>\n",
       "      <th>NUM_TRANSACTIONS_202202</th>\n",
       "      <th>NUM_TRANSACTIONS_202203</th>\n",
       "      <th>NUM_TRANSACTIONS_202204</th>\n",
       "      <th>NUM_TRANSACTIONS_202205</th>\n",
       "      <th>NUM_TRANSACTIONS_202206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>ZWZZ!W</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>130</td>\n",
       "      <td>100</td>\n",
       "      <td>68</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>&amp;WXYY&amp;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>Y%@YZ&amp;</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>!W%&amp;#!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>54</td>\n",
       "      <td>72</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>%##AXY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CREDIT_UNION_ID ACCOUNT_NUM  FLG_202201  FLG_202202  FLG_202203  FLG_202204  \\\n",
       "0               A      ZWZZ!W           1           1           1           1   \n",
       "1               A      &WXYY&           1           1           1           1   \n",
       "2               A      Y%@YZ&           1           1           1           1   \n",
       "3               A      !W%&#!           1           1           1           1   \n",
       "4               A      %##AXY           1           1           1           1   \n",
       "\n",
       "   FLG_202205  FLG_202206  DEEP_CHANNELS_202201  DEEP_CHANNELS_202202  ...  \\\n",
       "0           1           1                     0                     0  ...   \n",
       "1           1           1                     0                     0  ...   \n",
       "2           1           1                     0                     0  ...   \n",
       "3           1           1                     0                     0  ...   \n",
       "4           1           1                     0                     0  ...   \n",
       "\n",
       "   AMP_202203  AMP_202204  AMP_202205  AMP_202206  NUM_TRANSACTIONS_202201  \\\n",
       "0           3           3           2           4                       68   \n",
       "1           1           2           2           1                       14   \n",
       "2           2           2           1           2                       12   \n",
       "3           3           3           2           2                       38   \n",
       "4           3           2           3           2                       24   \n",
       "\n",
       "   NUM_TRANSACTIONS_202202  NUM_TRANSACTIONS_202203  NUM_TRANSACTIONS_202204  \\\n",
       "0                       86                      130                      100   \n",
       "1                        8                        4                       24   \n",
       "2                        6                       10                       12   \n",
       "3                       24                       36                       54   \n",
       "4                       12                       12                        8   \n",
       "\n",
       "   NUM_TRANSACTIONS_202205  NUM_TRANSACTIONS_202206  \n",
       "0                       68                      112  \n",
       "1                       12                       10  \n",
       "2                       12                       20  \n",
       "3                       72                       48  \n",
       "4                        6                       20  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = u.pd.read_csv('./Model Data Set (pseudo).csv',sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d114e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91848, 68)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3853474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make sure to create a copy of the data before we start altering it. Note that we don't change the original data we loaded.\n",
    "data = df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac166679",
   "metadata": {},
   "source": [
    "# Preparing Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672f29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare independent variables (X) and dependent variable (y)\n",
    "\n",
    "# To avoid writing them out every time, we save the names of the estimators of our model in a list. \n",
    "independent_variables=[#PIX\n",
    "            'DEEP_PIX_202201',\n",
    "            'DEEP_PIX_202202',\n",
    "            'DEEP_PIX_202203',\n",
    "            'DEEP_PIX_202204',\n",
    "            'DEEP_PIX_202205',\n",
    "            #BILLS\n",
    "            'DEEP_BILLS_202201',\n",
    "            'DEEP_BILLS_202202',\n",
    "            'DEEP_BILLS_202203',\n",
    "            'DEEP_BILLS_202204',\n",
    "            'DEEP_BILLS_202205',\n",
    "            #CARDS\n",
    "            'DEEP_CARDS_202201',\n",
    "            'DEEP_CARDS_202202',\n",
    "            'DEEP_CARDS_202203',\n",
    "            'DEEP_CARDS_202204',\n",
    "            'DEEP_CARDS_202205',\n",
    "            #CHECKING\n",
    "            'DEEP_CHECKING_202201',\n",
    "            'DEEP_CHECKING_202202',\n",
    "            'DEEP_CHECKING_202203',\n",
    "            'DEEP_CHECKING_202204',\n",
    "            'DEEP_CHECKING_202205',\n",
    "            #CREDIT\n",
    "            'DEEP_CREDIT_202201',\n",
    "            'DEEP_CREDIT_202202',\n",
    "            'DEEP_CREDIT_202203',\n",
    "            'DEEP_CREDIT_202204',\n",
    "            'DEEP_CREDIT_202205',\n",
    "            #INVESTMENTS\n",
    "            'DEEP_INVESTMENTS_202201',\n",
    "            'DEEP_INVESTMENTS_202202',\n",
    "            'DEEP_INVESTMENTS_202203',\n",
    "            'DEEP_INVESTMENTS_202204',\n",
    "            'DEEP_INVESTMENTS_202205',\n",
    "            #PAYMENTS\n",
    "            'DEEP_PAYMENTS_202201',\n",
    "            'DEEP_PAYMENTS_202202',\n",
    "            'DEEP_PAYMENTS_202203',\n",
    "            'DEEP_PAYMENTS_202204',\n",
    "            'DEEP_PAYMENTS_202205',\n",
    "            #AMPLITUDE\n",
    "            'AMP_202201',\n",
    "            'AMP_202202',\n",
    "            'AMP_202203',\n",
    "            'AMP_202204',\n",
    "            'AMP_202205'\n",
    "           ]\n",
    "\n",
    "X = data[independent_variables]\n",
    "y = data['FLG_202206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b485071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set shared model, scaler and splitter variables\n",
    "random_state = 26\n",
    "test_size = 0.15\n",
    "verbose = 'off'\n",
    "#set model names\n",
    "models = [#'Random Forest',\n",
    "          'Logistic Regression',\n",
    "         ]\n",
    "#set resampling method names\n",
    "resamplers = [\n",
    "              'Baseline',\n",
    "              'Random Over Sampling',\n",
    "              'SMOTE',\n",
    "              'Near Miss KNN',\n",
    "              'Random Under Sampling',\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56850fdd",
   "metadata": {},
   "source": [
    "# Handling class imbalance\n",
    "\n",
    "We know from our exploratory analysis that this dataset will be havily imbalanced with churn on 6th month as the minority class (represented as inactivity on that month or FLG_202206 = 0).\n",
    "\n",
    "The problem with classifiers and class imbalance is that the classifier will more easily classify the majority class, simply because most cases are of that class. For that reason model performance metrics have to be carefully selected. Precision, recall and F1 will be used as the main metrics for evaluating performance. In our specfic case we our most interested in those metrics regarding the prediction of the minority class (0 in our case).\n",
    "\n",
    "So in this study we will contrast the use of two wildly used classification models: Logistic Regression and RandomTreeClassifier, both with SciKit Learn implementations. Tree Ensembles our suposabily better at handling inbalance. And a common technique for getting better results is using resampling techniques. For that we will contrast model metrics on baseline models with resampled models (RandomOverSampling, SMOTE and NearMisses)\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://medium.com/grabngoinfo/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037\n",
    "\n",
    "https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bbdcf",
   "metadata": {},
   "source": [
    "## Processing and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e83623c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Baseline  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.01      0.02       423\n",
      "           1       0.97      1.00      0.98     13355\n",
      "\n",
      "    accuracy                           0.97     13778\n",
      "   macro avg       0.53      0.50      0.50     13778\n",
      "weighted avg       0.94      0.97      0.95     13778\n",
      "\n",
      "[[    5   418]\n",
      " [   54 13301]]\n",
      "\n",
      "Total processing time: --- 11.438204526901245 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Random Over Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.02      0.03       423\n",
      "           1       0.97      0.99      0.98     13355\n",
      "\n",
      "    accuracy                           0.96     13778\n",
      "   macro avg       0.53      0.51      0.51     13778\n",
      "weighted avg       0.94      0.96      0.95     13778\n",
      "\n",
      "[[    9   414]\n",
      " [   89 13266]]\n",
      "\n",
      "Total processing time: --- 22.614874601364136 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with SMOTE  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.14      0.14       423\n",
      "           1       0.97      0.97      0.97     13355\n",
      "\n",
      "    accuracy                           0.95     13778\n",
      "   macro avg       0.55      0.56      0.55     13778\n",
      "weighted avg       0.95      0.95      0.95     13778\n",
      "\n",
      "[[   60   363]\n",
      " [  391 12964]]\n",
      "\n",
      "Total processing time: --- 25.52273654937744 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Near Miss KNN  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.01      0.02       423\n",
      "           1       0.97      1.00      0.98     13355\n",
      "\n",
      "    accuracy                           0.97     13778\n",
      "   macro avg       0.53      0.50      0.50     13778\n",
      "weighted avg       0.94      0.97      0.95     13778\n",
      "\n",
      "[[    5   418]\n",
      " [   54 13301]]\n",
      "\n",
      "Total processing time: --- 10.381755113601685 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Random Under Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.77      0.16       423\n",
      "           1       0.99      0.75      0.85     13355\n",
      "\n",
      "    accuracy                           0.75     13778\n",
      "   macro avg       0.54      0.76      0.51     13778\n",
      "weighted avg       0.96      0.75      0.83     13778\n",
      "\n",
      "[[  327    96]\n",
      " [ 3337 10018]]\n",
      "\n",
      "Total processing time: --- 0.8806493282318115 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Baseline  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.69      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.77      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 356   67]\n",
      " [4139 9216]]\n",
      "\n",
      "Total processing time: --- 2.1065797805786133 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Random Over Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.15       423\n",
      "           1       0.99      0.69      0.82     13355\n",
      "\n",
      "    accuracy                           0.70     13778\n",
      "   macro avg       0.54      0.77      0.48     13778\n",
      "weighted avg       0.96      0.70      0.80     13778\n",
      "\n",
      "[[ 357   66]\n",
      " [4101 9254]]\n",
      "\n",
      "Total processing time: --- 3.615339756011963 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with SMOTE  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.15       423\n",
      "           1       0.99      0.70      0.82     13355\n",
      "\n",
      "    accuracy                           0.71     13778\n",
      "   macro avg       0.54      0.77      0.49     13778\n",
      "weighted avg       0.96      0.71      0.80     13778\n",
      "\n",
      "[[ 356   67]\n",
      " [3960 9395]]\n",
      "\n",
      "Total processing time: --- 4.554826021194458 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Near Miss KNN  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.69      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.77      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 356   67]\n",
      " [4139 9216]]\n",
      "\n",
      "Total processing time: --- 2.443470001220703 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Random Under Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.83      0.14       423\n",
      "           1       0.99      0.68      0.81     13355\n",
      "\n",
      "    accuracy                           0.68     13778\n",
      "   macro avg       0.53      0.75      0.47     13778\n",
      "weighted avg       0.96      0.68      0.79     13778\n",
      "\n",
      "[[ 349   74]\n",
      " [4290 9065]]\n",
      "\n",
      "Total processing time: --- 0.2932169437408447 seconds ---\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26, solver='saga',\n",
      "                   tol=0.01)\n",
      "LogisticRegression(class_weight='balanced', random_state=26, solver='saga',\n",
      "                   tol=0.01)\n",
      "LogisticRegression(class_weight='balanced', random_state=26, solver='saga',\n",
      "                   tol=0.01)\n",
      "LogisticRegression(class_weight='balanced', random_state=26, solver='saga',\n",
      "                   tol=0.01)\n",
      "LogisticRegression(class_weight='balanced', random_state=26, solver='saga',\n",
      "                   tol=0.01)\n"
     ]
    }
   ],
   "source": [
    "#Instantiate empty list to hold models for coeficient evaluation \n",
    "model_prediction = []\n",
    "\n",
    "model_scores_table = u.pd.DataFrame()\n",
    "model_scores_table['Scores'] = ['Model','Model Name','Resampler Name','TN','FP','FN','TP','Precision 0','Precision 1','Recall 0','Recall 1','F1-Score 0','F1-Score 1','Support 0','Support 1']\n",
    "\n",
    "for model_name in models:\n",
    "    for resampler_name in resamplers:\n",
    "        model,cr,cm,precision,recall,fbeta_score,support = u.model_predict(model_name,resampler_name,X,y,random_state,test_size,verbose)\n",
    "        models_scores_append = [model,model_name,resampler_name,cm[0][0],cm[0][1],cm[1][0],cm[1][1],precision[0],precision[1],recall[0],recall[1],fbeta_score[0],fbeta_score[1],support[0],support[1]]\n",
    "        \n",
    "        model_scores_table[model_name+' '+resampler_name] = models_scores_append\n",
    "        model_prediction.append((model,model_name,resampler_name))\n",
    "\n",
    "# Create Model Coeficient Table\n",
    "Model_Coef_Table = u.Model_Coef_Table(model_prediction, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d588985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "import utils as u\n",
    "\n",
    "u = reload(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc2319",
   "metadata": {},
   "source": [
    "# Section 5: Evaluate the Results\n",
    "\n",
    "This section will split up into separate analyses for each business question.\n",
    "Each section will be comprised of a brief analysis, and evaluation and conclusion.\n",
    "\n",
    "### Question 2: Are mono-product-family users more likely to have channel innactivity in a six month window?\n",
    "\n",
    "### Question 3: Can transactional data alone safely predict channel innactivity in a six month window?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6bb1b",
   "metadata": {},
   "source": [
    "## Question 1: What are aspects of a transactional dataset that can be used for understanding channel innactivity in a six month window?\n",
    "\n",
    "The objetctive behind this question is to understand what predictors from our transactional have the higest impact on model performance.\n",
    "\n",
    "We will:\n",
    "\n",
    "(1) Check de Model Scores to identify best models;\n",
    "\n",
    "(2) We will verify which features had greater impact on our best models;\n",
    "\n",
    "(3) We will then reavaluate our models using only the best predictor to check if performance boosts up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e352dd",
   "metadata": {},
   "source": [
    "##### Quick Primer com classifier evaluation scores\n",
    "\n",
    "Quick Primer on reading the Confusion Matrix and Classification report measures\n",
    "How to read the quadrants of the matrix:\n",
    "\n",
    "True Negative | False Positive\n",
    "\n",
    "False Negative | True Positive\n",
    "\n",
    "Precision\n",
    "Measure of how many of the positive predictions made are correct (true positives).\n",
    "Formula: TP/(TP+FP)\n",
    "\n",
    "Recall\n",
    "Measure of how many of the positive cases the classifier correctly predicted considering the over all positive cases in the data.\n",
    "It is sometimes also referred to as Sensitivity\n",
    "Formula: TP/(TP+FN)\n",
    "\n",
    "f1-Score\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "Accuracy\n",
    "Measure of the number of correct predictions over all predictions\n",
    "Formula: (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7b1c518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Resampler Name</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Precision 0</th>\n",
       "      <th>Recall 0</th>\n",
       "      <th>F1-Score 0</th>\n",
       "      <th>Support 0</th>\n",
       "      <th>Support 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>5</td>\n",
       "      <td>418</td>\n",
       "      <td>54</td>\n",
       "      <td>13301</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.01182</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Random Over Sampling</td>\n",
       "      <td>9</td>\n",
       "      <td>414</td>\n",
       "      <td>89</td>\n",
       "      <td>13266</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.034549</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>60</td>\n",
       "      <td>363</td>\n",
       "      <td>391</td>\n",
       "      <td>12964</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.141844</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Near Miss KNN</td>\n",
       "      <td>5</td>\n",
       "      <td>418</td>\n",
       "      <td>54</td>\n",
       "      <td>13301</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.01182</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Random Under Sampling</td>\n",
       "      <td>327</td>\n",
       "      <td>96</td>\n",
       "      <td>3337</td>\n",
       "      <td>10018</td>\n",
       "      <td>0.089247</td>\n",
       "      <td>0.77305</td>\n",
       "      <td>0.16002</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(class_weight='balanced', ra...</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>356</td>\n",
       "      <td>67</td>\n",
       "      <td>4139</td>\n",
       "      <td>9216</td>\n",
       "      <td>0.079199</td>\n",
       "      <td>0.841608</td>\n",
       "      <td>0.144774</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(class_weight='balanced', ra...</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Random Over Sampling</td>\n",
       "      <td>357</td>\n",
       "      <td>66</td>\n",
       "      <td>4101</td>\n",
       "      <td>9254</td>\n",
       "      <td>0.080081</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>0.146281</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(class_weight='balanced', ra...</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>356</td>\n",
       "      <td>67</td>\n",
       "      <td>3960</td>\n",
       "      <td>9395</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>0.841608</td>\n",
       "      <td>0.150243</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(class_weight='balanced', ra...</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Near Miss KNN</td>\n",
       "      <td>356</td>\n",
       "      <td>67</td>\n",
       "      <td>4139</td>\n",
       "      <td>9216</td>\n",
       "      <td>0.079199</td>\n",
       "      <td>0.841608</td>\n",
       "      <td>0.144774</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(class_weight='balanced', ra...</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Random Under Sampling</td>\n",
       "      <td>349</td>\n",
       "      <td>74</td>\n",
       "      <td>4290</td>\n",
       "      <td>9065</td>\n",
       "      <td>0.075232</td>\n",
       "      <td>0.825059</td>\n",
       "      <td>0.13789</td>\n",
       "      <td>423</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model           Model Name  \\\n",
       "0  (DecisionTreeClassifier(max_features='sqrt', r...        Random Forest   \n",
       "1  (DecisionTreeClassifier(max_features='sqrt', r...        Random Forest   \n",
       "2  (DecisionTreeClassifier(max_features='sqrt', r...        Random Forest   \n",
       "3  (DecisionTreeClassifier(max_features='sqrt', r...        Random Forest   \n",
       "4  (DecisionTreeClassifier(max_features='sqrt', r...        Random Forest   \n",
       "5  LogisticRegression(class_weight='balanced', ra...  Logistic Regression   \n",
       "6  LogisticRegression(class_weight='balanced', ra...  Logistic Regression   \n",
       "7  LogisticRegression(class_weight='balanced', ra...  Logistic Regression   \n",
       "8  LogisticRegression(class_weight='balanced', ra...  Logistic Regression   \n",
       "9  LogisticRegression(class_weight='balanced', ra...  Logistic Regression   \n",
       "\n",
       "          Resampler Name   TN   FP    FN     TP Precision 0  Recall 0  \\\n",
       "0               Baseline    5  418    54  13301    0.084746   0.01182   \n",
       "1   Random Over Sampling    9  414    89  13266    0.091837  0.021277   \n",
       "2                  SMOTE   60  363   391  12964    0.133038  0.141844   \n",
       "3          Near Miss KNN    5  418    54  13301    0.084746   0.01182   \n",
       "4  Random Under Sampling  327   96  3337  10018    0.089247   0.77305   \n",
       "5               Baseline  356   67  4139   9216    0.079199  0.841608   \n",
       "6   Random Over Sampling  357   66  4101   9254    0.080081  0.843972   \n",
       "7                  SMOTE  356   67  3960   9395    0.082484  0.841608   \n",
       "8          Near Miss KNN  356   67  4139   9216    0.079199  0.841608   \n",
       "9  Random Under Sampling  349   74  4290   9065    0.075232  0.825059   \n",
       "\n",
       "  F1-Score 0 Support 0 Support 1  \n",
       "0   0.020747       423     13355  \n",
       "1   0.034549       423     13355  \n",
       "2     0.1373       423     13355  \n",
       "3   0.020747       423     13355  \n",
       "4    0.16002       423     13355  \n",
       "5   0.144774       423     13355  \n",
       "6   0.146281       423     13355  \n",
       "7   0.150243       423     13355  \n",
       "8   0.144774       423     13355  \n",
       "9    0.13789       423     13355  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's transpose the Model Scores Table to get a better look\n",
    "model_scores_table_T = model_scores_table.T\n",
    "# Now let's promote the first row as header and drop the index\n",
    "model_scores_table_T = model_scores_table_T.rename(columns=model_scores_table_T.iloc[0]).drop(model_scores_table_T.index[0]).reset_index(drop=True)\n",
    "# Let's clean out the score related to predicint the majority class (1)\n",
    "model_scores_table_T = model_scores_table_T.drop(columns=['Precision 1','Recall 1','F1-Score 1'])\n",
    "model_scores_table_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "56df1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: \n",
      "TN                  357\n",
      "FP                  418\n",
      "FN                 4290\n",
      "TP                13301\n",
      "Precision 0    0.133038\n",
      "Precision 1    0.992919\n",
      "Recall 0       0.843972\n",
      "Recall 1       0.995957\n",
      "F1-Score 0      0.16002\n",
      "F1-Score 1     0.982566\n",
      "Support 0           423\n",
      "Support 1         13355\n",
      "dtype: object\n",
      "\n",
      "Minimum: \n",
      "TN                    5\n",
      "FP                   66\n",
      "FN                   54\n",
      "TP                 9065\n",
      "Precision 0    0.075232\n",
      "Precision 1    0.969531\n",
      "Recall 0        0.01182\n",
      "Recall 1       0.678772\n",
      "F1-Score 0     0.020747\n",
      "F1-Score 1     0.805993\n",
      "Support 0           423\n",
      "Support 1         13355\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Maximum: ')\n",
    "print(model_scores_table_T.drop(columns=['Model','Model Name','Resampler Name']).max(axis = 0))\n",
    "print('\\nMinimum: ')\n",
    "print(model_scores_table_T.drop(columns=['Model','Model Name','Resampler Name']).min(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e5872",
   "metadata": {},
   "source": [
    "## Classfication Score analysis\n",
    "\n",
    "All models had precision scores for the minority class (Precision 0) ranging from 0.07 to 0.13, recall from 0.01 to 0.85 and f1-score from 0.02 to 0.16. This is actually a rather distressing signal that, in overall, none of the models did a remarkable job at predicting innactivity. They were pretty simple bad at it.\n",
    "\n",
    "One of most interesting difference between models are seeable through confusion matrices' True Negatives, False Positives, False Negatives and True Positives scores. \n",
    "\n",
    "The best case scenario shows that the models actually did an interesting job of predicting 357 cases (check Max True Negatives) of the 423 (check Support 0) innactivity targets in the test set. That score was achieved by Logistic Regression with Random Under Sampling model, which,not surpriselingy, had also the higest Recall 0.\n",
    "\n",
    "This means the model is more confident at trying to predict the minority cases. There are a few models that had really low negative predictions as whole like the Random Forest Baseline and Near Miss KNN who score lowest practically didn’t even try to predict the minority cases (only 0,04% of predictions were for the minority class).\n",
    "\n",
    "For this step of the process we will elect the Models with higher Minority cases predictions. We see their Recall scores are high, but F1-Score and Precision are low. Further studies can look it fine tunning these models to try to reduce the False Negative scores, maybe using differnt class weights, penalization and solver methods.\n",
    "\n",
    "To continue on this analysis I will choose the two models with high True Negatives but with their pairs lowest False Negative predictions. These are the Random Forest with Random Under Sampling and Logistic Regression with SMOTE. We will look their coeficients to get a feel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7430157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>CoefRandom ForestBaseline</th>\n",
       "      <th>CoefRandom ForestRandom Over Sampling</th>\n",
       "      <th>CoefRandom ForestSMOTE</th>\n",
       "      <th>CoefRandom ForestNear Miss KNN</th>\n",
       "      <th>CoefRandom ForestRandom Under Sampling</th>\n",
       "      <th>CoefLogistic RegressionBaseline</th>\n",
       "      <th>CoefLogistic RegressionRandom Over Sampling</th>\n",
       "      <th>CoefLogistic RegressionSMOTE</th>\n",
       "      <th>CoefLogistic RegressionNear Miss KNN</th>\n",
       "      <th>CoefLogistic RegressionRandom Under Sampling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEEP_PIX_202201</td>\n",
       "      <td>0.059410</td>\n",
       "      <td>0.059683</td>\n",
       "      <td>0.047388</td>\n",
       "      <td>0.059410</td>\n",
       "      <td>0.055802</td>\n",
       "      <td>-0.108326</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>-0.134778</td>\n",
       "      <td>-0.108326</td>\n",
       "      <td>-0.195564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEEP_PIX_202202</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.057625</td>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.054875</td>\n",
       "      <td>-0.244472</td>\n",
       "      <td>-0.257599</td>\n",
       "      <td>-0.288997</td>\n",
       "      <td>-0.244472</td>\n",
       "      <td>-0.196759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEEP_PIX_202203</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.066350</td>\n",
       "      <td>0.050336</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.060874</td>\n",
       "      <td>-0.296879</td>\n",
       "      <td>-0.290820</td>\n",
       "      <td>-0.303673</td>\n",
       "      <td>-0.296879</td>\n",
       "      <td>-0.315867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEEP_PIX_202204</td>\n",
       "      <td>0.076184</td>\n",
       "      <td>0.071977</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>0.076184</td>\n",
       "      <td>0.073048</td>\n",
       "      <td>-0.206332</td>\n",
       "      <td>-0.210498</td>\n",
       "      <td>-0.226016</td>\n",
       "      <td>-0.206332</td>\n",
       "      <td>-0.202132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEEP_PIX_202205</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.151696</td>\n",
       "      <td>0.123374</td>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.150976</td>\n",
       "      <td>2.584145</td>\n",
       "      <td>2.573109</td>\n",
       "      <td>3.268187</td>\n",
       "      <td>2.584145</td>\n",
       "      <td>2.595444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEEP_BILLS_202201</td>\n",
       "      <td>0.018865</td>\n",
       "      <td>0.019921</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.018865</td>\n",
       "      <td>0.018165</td>\n",
       "      <td>-0.149018</td>\n",
       "      <td>-0.159326</td>\n",
       "      <td>-0.145370</td>\n",
       "      <td>-0.149018</td>\n",
       "      <td>-0.257109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DEEP_BILLS_202202</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.017776</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>-0.071881</td>\n",
       "      <td>-0.065141</td>\n",
       "      <td>-0.046134</td>\n",
       "      <td>-0.071881</td>\n",
       "      <td>-0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DEEP_BILLS_202203</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.017260</td>\n",
       "      <td>-0.103426</td>\n",
       "      <td>-0.109691</td>\n",
       "      <td>-0.152300</td>\n",
       "      <td>-0.103426</td>\n",
       "      <td>-0.115335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DEEP_BILLS_202204</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.135689</td>\n",
       "      <td>0.136148</td>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.126297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DEEP_BILLS_202205</td>\n",
       "      <td>0.037868</td>\n",
       "      <td>0.039010</td>\n",
       "      <td>0.040325</td>\n",
       "      <td>0.037868</td>\n",
       "      <td>0.042316</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.462249</td>\n",
       "      <td>0.669784</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.483864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DEEP_CARDS_202201</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>-0.090709</td>\n",
       "      <td>-0.093517</td>\n",
       "      <td>-0.093510</td>\n",
       "      <td>-0.090709</td>\n",
       "      <td>-0.075795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DEEP_CARDS_202202</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>0.006617</td>\n",
       "      <td>-0.107304</td>\n",
       "      <td>-0.118882</td>\n",
       "      <td>-0.099994</td>\n",
       "      <td>-0.107304</td>\n",
       "      <td>-0.120734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DEEP_CARDS_202203</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>-0.116127</td>\n",
       "      <td>-0.109860</td>\n",
       "      <td>-0.151555</td>\n",
       "      <td>-0.116127</td>\n",
       "      <td>-0.153314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DEEP_CARDS_202204</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>-0.072756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DEEP_CARDS_202205</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.007765</td>\n",
       "      <td>0.081322</td>\n",
       "      <td>0.085111</td>\n",
       "      <td>0.188342</td>\n",
       "      <td>0.081322</td>\n",
       "      <td>0.120987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DEEP_CHECKING_202201</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.015255</td>\n",
       "      <td>-0.037000</td>\n",
       "      <td>-0.023898</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>-0.037000</td>\n",
       "      <td>0.016452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DEEP_CHECKING_202202</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.014770</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>-0.034097</td>\n",
       "      <td>-0.030456</td>\n",
       "      <td>-0.029332</td>\n",
       "      <td>-0.034097</td>\n",
       "      <td>-0.116972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEEP_CHECKING_202203</td>\n",
       "      <td>0.016237</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.016237</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>-0.025280</td>\n",
       "      <td>-0.019646</td>\n",
       "      <td>-0.041103</td>\n",
       "      <td>-0.025280</td>\n",
       "      <td>-0.019712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DEEP_CHECKING_202204</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>-0.015290</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>-0.027544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DEEP_CHECKING_202205</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>0.016287</td>\n",
       "      <td>0.159221</td>\n",
       "      <td>0.146049</td>\n",
       "      <td>0.259696</td>\n",
       "      <td>0.159221</td>\n",
       "      <td>0.165601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DEEP_CREDIT_202201</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.011286</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>-0.074422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DEEP_CREDIT_202202</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>-0.049315</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DEEP_CREDIT_202203</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>-0.116221</td>\n",
       "      <td>-0.104685</td>\n",
       "      <td>-0.107868</td>\n",
       "      <td>-0.116221</td>\n",
       "      <td>-0.162969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DEEP_CREDIT_202204</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.090829</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>-0.038755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DEEP_CREDIT_202205</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>-0.052486</td>\n",
       "      <td>-0.060258</td>\n",
       "      <td>-0.036002</td>\n",
       "      <td>-0.052486</td>\n",
       "      <td>0.077327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DEEP_INVESTMENTS_202201</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.105754</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.068165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DEEP_INVESTMENTS_202202</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>-0.082068</td>\n",
       "      <td>-0.080351</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>-0.082068</td>\n",
       "      <td>-0.073575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DEEP_INVESTMENTS_202203</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.007726</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.076750</td>\n",
       "      <td>0.068483</td>\n",
       "      <td>0.143658</td>\n",
       "      <td>0.076750</td>\n",
       "      <td>0.077587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DEEP_INVESTMENTS_202204</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.005521</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>-0.087735</td>\n",
       "      <td>-0.065489</td>\n",
       "      <td>-0.106530</td>\n",
       "      <td>-0.087735</td>\n",
       "      <td>-0.127131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DEEP_INVESTMENTS_202205</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.042243</td>\n",
       "      <td>0.031559</td>\n",
       "      <td>0.094669</td>\n",
       "      <td>0.042243</td>\n",
       "      <td>0.064757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DEEP_PAYMENTS_202201</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.019804</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>-0.028000</td>\n",
       "      <td>-0.017324</td>\n",
       "      <td>-0.017928</td>\n",
       "      <td>-0.028000</td>\n",
       "      <td>-0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>DEEP_PAYMENTS_202202</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>-0.236646</td>\n",
       "      <td>-0.258015</td>\n",
       "      <td>-0.241094</td>\n",
       "      <td>-0.236646</td>\n",
       "      <td>-0.204979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>DEEP_PAYMENTS_202203</td>\n",
       "      <td>0.021163</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.021964</td>\n",
       "      <td>0.021163</td>\n",
       "      <td>0.021554</td>\n",
       "      <td>-0.108839</td>\n",
       "      <td>-0.100305</td>\n",
       "      <td>-0.105473</td>\n",
       "      <td>-0.108839</td>\n",
       "      <td>-0.161267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>DEEP_PAYMENTS_202204</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.071536</td>\n",
       "      <td>0.046455</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>-0.021777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>DEEP_PAYMENTS_202205</td>\n",
       "      <td>0.051003</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>0.051003</td>\n",
       "      <td>0.054880</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.795888</td>\n",
       "      <td>1.235656</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.847823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AMP_202201</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.027431</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>0.020324</td>\n",
       "      <td>0.066914</td>\n",
       "      <td>0.068997</td>\n",
       "      <td>0.020119</td>\n",
       "      <td>0.066914</td>\n",
       "      <td>0.074611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>AMP_202202</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.027043</td>\n",
       "      <td>0.041672</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.027963</td>\n",
       "      <td>0.251349</td>\n",
       "      <td>0.255529</td>\n",
       "      <td>0.247212</td>\n",
       "      <td>0.251349</td>\n",
       "      <td>0.265332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AMP_202203</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.025645</td>\n",
       "      <td>0.027968</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.099367</td>\n",
       "      <td>0.098762</td>\n",
       "      <td>0.077270</td>\n",
       "      <td>0.099367</td>\n",
       "      <td>0.116973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AMP_202204</td>\n",
       "      <td>0.037761</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>0.069552</td>\n",
       "      <td>0.037761</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.172331</td>\n",
       "      <td>0.172829</td>\n",
       "      <td>0.149252</td>\n",
       "      <td>0.172331</td>\n",
       "      <td>0.198556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AMP_202205</td>\n",
       "      <td>0.074197</td>\n",
       "      <td>0.068257</td>\n",
       "      <td>0.114005</td>\n",
       "      <td>0.074197</td>\n",
       "      <td>0.077617</td>\n",
       "      <td>0.463255</td>\n",
       "      <td>0.459834</td>\n",
       "      <td>0.423388</td>\n",
       "      <td>0.463255</td>\n",
       "      <td>0.513159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Features  CoefRandom ForestBaseline  \\\n",
       "0           DEEP_PIX_202201                   0.059410   \n",
       "1           DEEP_PIX_202202                   0.057143   \n",
       "2           DEEP_PIX_202203                   0.065239   \n",
       "3           DEEP_PIX_202204                   0.076184   \n",
       "4           DEEP_PIX_202205                   0.147647   \n",
       "5         DEEP_BILLS_202201                   0.018865   \n",
       "6         DEEP_BILLS_202202                   0.017776   \n",
       "7         DEEP_BILLS_202203                   0.018481   \n",
       "8         DEEP_BILLS_202204                   0.020911   \n",
       "9         DEEP_BILLS_202205                   0.037868   \n",
       "10        DEEP_CARDS_202201                   0.006334   \n",
       "11        DEEP_CARDS_202202                   0.006859   \n",
       "12        DEEP_CARDS_202203                   0.007423   \n",
       "13        DEEP_CARDS_202204                   0.006826   \n",
       "14        DEEP_CARDS_202205                   0.007208   \n",
       "15     DEEP_CHECKING_202201                   0.015060   \n",
       "16     DEEP_CHECKING_202202                   0.014835   \n",
       "17     DEEP_CHECKING_202203                   0.016237   \n",
       "18     DEEP_CHECKING_202204                   0.014074   \n",
       "19     DEEP_CHECKING_202205                   0.017558   \n",
       "20       DEEP_CREDIT_202201                   0.002446   \n",
       "21       DEEP_CREDIT_202202                   0.002377   \n",
       "22       DEEP_CREDIT_202203                   0.003311   \n",
       "23       DEEP_CREDIT_202204                   0.002555   \n",
       "24       DEEP_CREDIT_202205                   0.002356   \n",
       "25  DEEP_INVESTMENTS_202201                   0.006897   \n",
       "26  DEEP_INVESTMENTS_202202                   0.006354   \n",
       "27  DEEP_INVESTMENTS_202203                   0.007313   \n",
       "28  DEEP_INVESTMENTS_202204                   0.006693   \n",
       "29  DEEP_INVESTMENTS_202205                   0.006910   \n",
       "30     DEEP_PAYMENTS_202201                   0.019566   \n",
       "31     DEEP_PAYMENTS_202202                   0.018600   \n",
       "32     DEEP_PAYMENTS_202203                   0.021163   \n",
       "33     DEEP_PAYMENTS_202204                   0.024108   \n",
       "34     DEEP_PAYMENTS_202205                   0.051003   \n",
       "35               AMP_202201                   0.022199   \n",
       "36               AMP_202202                   0.027359   \n",
       "37               AMP_202203                   0.024894   \n",
       "38               AMP_202204                   0.037761   \n",
       "39               AMP_202205                   0.074197   \n",
       "\n",
       "    CoefRandom ForestRandom Over Sampling  CoefRandom ForestSMOTE  \\\n",
       "0                                0.059683                0.047388   \n",
       "1                                0.057625                0.045647   \n",
       "2                                0.066350                0.050336   \n",
       "3                                0.071977                0.056631   \n",
       "4                                0.151696                0.123374   \n",
       "5                                0.019921                0.017487   \n",
       "6                                0.017891                0.015607   \n",
       "7                                0.018387                0.016546   \n",
       "8                                0.020640                0.023597   \n",
       "9                                0.039010                0.040325   \n",
       "10                               0.006528                0.006738   \n",
       "11                               0.006666                0.006880   \n",
       "12                               0.007062                0.007604   \n",
       "13                               0.006750                0.006895   \n",
       "14                               0.007257                0.008012   \n",
       "15                               0.015691                0.012246   \n",
       "16                               0.014770                0.012085   \n",
       "17                               0.015940                0.013171   \n",
       "18                               0.014309                0.012938   \n",
       "19                               0.017489                0.016326   \n",
       "20                               0.002517                0.001828   \n",
       "21                               0.002273                0.001624   \n",
       "22                               0.003351                0.002916   \n",
       "23                               0.002385                0.001995   \n",
       "24                               0.002273                0.001783   \n",
       "25                               0.007207                0.005478   \n",
       "26                               0.006413                0.004459   \n",
       "27                               0.007726                0.005550   \n",
       "28                               0.006898                0.005521   \n",
       "29                               0.007165                0.006365   \n",
       "30                               0.019804                0.020073   \n",
       "31                               0.017880                0.016418   \n",
       "32                               0.021778                0.021964   \n",
       "33                               0.025187                0.022741   \n",
       "34                               0.049682                0.060822   \n",
       "35                               0.022224                0.027431   \n",
       "36                               0.027043                0.041672   \n",
       "37                               0.025645                0.027968   \n",
       "38                               0.038650                0.069552   \n",
       "39                               0.068257                0.114005   \n",
       "\n",
       "    CoefRandom ForestNear Miss KNN  CoefRandom ForestRandom Under Sampling  \\\n",
       "0                         0.059410                                0.055802   \n",
       "1                         0.057143                                0.054875   \n",
       "2                         0.065239                                0.060874   \n",
       "3                         0.076184                                0.073048   \n",
       "4                         0.147647                                0.150976   \n",
       "5                         0.018865                                0.018165   \n",
       "6                         0.017776                                0.018118   \n",
       "7                         0.018481                                0.017260   \n",
       "8                         0.020911                                0.019468   \n",
       "9                         0.037868                                0.042316   \n",
       "10                        0.006334                                0.006904   \n",
       "11                        0.006859                                0.006617   \n",
       "12                        0.007423                                0.007490   \n",
       "13                        0.006826                                0.007241   \n",
       "14                        0.007208                                0.007765   \n",
       "15                        0.015060                                0.015255   \n",
       "16                        0.014835                                0.013924   \n",
       "17                        0.016237                                0.015694   \n",
       "18                        0.014074                                0.014608   \n",
       "19                        0.017558                                0.016287   \n",
       "20                        0.002446                                0.002088   \n",
       "21                        0.002377                                0.002199   \n",
       "22                        0.003311                                0.003688   \n",
       "23                        0.002555                                0.002252   \n",
       "24                        0.002356                                0.002654   \n",
       "25                        0.006897                                0.007326   \n",
       "26                        0.006354                                0.007025   \n",
       "27                        0.007313                                0.007479   \n",
       "28                        0.006693                                0.007868   \n",
       "29                        0.006910                                0.007822   \n",
       "30                        0.019566                                0.019375   \n",
       "31                        0.018600                                0.018166   \n",
       "32                        0.021163                                0.021554   \n",
       "33                        0.024108                                0.025297   \n",
       "34                        0.051003                                0.054880   \n",
       "35                        0.022199                                0.020324   \n",
       "36                        0.027359                                0.027963   \n",
       "37                        0.024894                                0.025754   \n",
       "38                        0.037761                                0.035980   \n",
       "39                        0.074197                                0.077617   \n",
       "\n",
       "    CoefLogistic RegressionBaseline  \\\n",
       "0                         -0.108326   \n",
       "1                         -0.244472   \n",
       "2                         -0.296879   \n",
       "3                         -0.206332   \n",
       "4                          2.584145   \n",
       "5                         -0.149018   \n",
       "6                         -0.071881   \n",
       "7                         -0.103426   \n",
       "8                          0.126906   \n",
       "9                          0.451890   \n",
       "10                        -0.090709   \n",
       "11                        -0.107304   \n",
       "12                        -0.116127   \n",
       "13                         0.009170   \n",
       "14                         0.081322   \n",
       "15                        -0.037000   \n",
       "16                        -0.034097   \n",
       "17                        -0.025280   \n",
       "18                        -0.006349   \n",
       "19                         0.159221   \n",
       "20                         0.004388   \n",
       "21                        -0.044407   \n",
       "22                        -0.116221   \n",
       "23                         0.024691   \n",
       "24                        -0.052486   \n",
       "25                         0.055936   \n",
       "26                        -0.082068   \n",
       "27                         0.076750   \n",
       "28                        -0.087735   \n",
       "29                         0.042243   \n",
       "30                        -0.028000   \n",
       "31                        -0.236646   \n",
       "32                        -0.108839   \n",
       "33                         0.056497   \n",
       "34                         0.788378   \n",
       "35                         0.066914   \n",
       "36                         0.251349   \n",
       "37                         0.099367   \n",
       "38                         0.172331   \n",
       "39                         0.463255   \n",
       "\n",
       "    CoefLogistic RegressionRandom Over Sampling  CoefLogistic RegressionSMOTE  \\\n",
       "0                                     -0.094238                     -0.134778   \n",
       "1                                     -0.257599                     -0.288997   \n",
       "2                                     -0.290820                     -0.303673   \n",
       "3                                     -0.210498                     -0.226016   \n",
       "4                                      2.573109                      3.268187   \n",
       "5                                     -0.159326                     -0.145370   \n",
       "6                                     -0.065141                     -0.046134   \n",
       "7                                     -0.109691                     -0.152300   \n",
       "8                                      0.135689                      0.136148   \n",
       "9                                      0.462249                      0.669784   \n",
       "10                                    -0.093517                     -0.093510   \n",
       "11                                    -0.118882                     -0.099994   \n",
       "12                                    -0.109860                     -0.151555   \n",
       "13                                     0.018097                      0.052700   \n",
       "14                                     0.085111                      0.188342   \n",
       "15                                    -0.023898                      0.063620   \n",
       "16                                    -0.030456                     -0.029332   \n",
       "17                                    -0.019646                     -0.041103   \n",
       "18                                    -0.015290                      0.001035   \n",
       "19                                     0.146049                      0.259696   \n",
       "20                                     0.015941                      0.011286   \n",
       "21                                    -0.049315                     -0.031082   \n",
       "22                                    -0.104685                     -0.107868   \n",
       "23                                     0.008275                      0.090829   \n",
       "24                                    -0.060258                     -0.036002   \n",
       "25                                     0.044722                      0.105754   \n",
       "26                                    -0.080351                     -0.064941   \n",
       "27                                     0.068483                      0.143658   \n",
       "28                                    -0.065489                     -0.106530   \n",
       "29                                     0.031559                      0.094669   \n",
       "30                                    -0.017324                     -0.017928   \n",
       "31                                    -0.258015                     -0.241094   \n",
       "32                                    -0.100305                     -0.105473   \n",
       "33                                     0.071536                      0.046455   \n",
       "34                                     0.795888                      1.235656   \n",
       "35                                     0.068997                      0.020119   \n",
       "36                                     0.255529                      0.247212   \n",
       "37                                     0.098762                      0.077270   \n",
       "38                                     0.172829                      0.149252   \n",
       "39                                     0.459834                      0.423388   \n",
       "\n",
       "    CoefLogistic RegressionNear Miss KNN  \\\n",
       "0                              -0.108326   \n",
       "1                              -0.244472   \n",
       "2                              -0.296879   \n",
       "3                              -0.206332   \n",
       "4                               2.584145   \n",
       "5                              -0.149018   \n",
       "6                              -0.071881   \n",
       "7                              -0.103426   \n",
       "8                               0.126906   \n",
       "9                               0.451890   \n",
       "10                             -0.090709   \n",
       "11                             -0.107304   \n",
       "12                             -0.116127   \n",
       "13                              0.009170   \n",
       "14                              0.081322   \n",
       "15                             -0.037000   \n",
       "16                             -0.034097   \n",
       "17                             -0.025280   \n",
       "18                             -0.006349   \n",
       "19                              0.159221   \n",
       "20                              0.004388   \n",
       "21                             -0.044407   \n",
       "22                             -0.116221   \n",
       "23                              0.024691   \n",
       "24                             -0.052486   \n",
       "25                              0.055936   \n",
       "26                             -0.082068   \n",
       "27                              0.076750   \n",
       "28                             -0.087735   \n",
       "29                              0.042243   \n",
       "30                             -0.028000   \n",
       "31                             -0.236646   \n",
       "32                             -0.108839   \n",
       "33                              0.056497   \n",
       "34                              0.788378   \n",
       "35                              0.066914   \n",
       "36                              0.251349   \n",
       "37                              0.099367   \n",
       "38                              0.172331   \n",
       "39                              0.463255   \n",
       "\n",
       "    CoefLogistic RegressionRandom Under Sampling  \n",
       "0                                      -0.195564  \n",
       "1                                      -0.196759  \n",
       "2                                      -0.315867  \n",
       "3                                      -0.202132  \n",
       "4                                       2.595444  \n",
       "5                                      -0.257109  \n",
       "6                                      -0.062800  \n",
       "7                                      -0.115335  \n",
       "8                                       0.126297  \n",
       "9                                       0.483864  \n",
       "10                                     -0.075795  \n",
       "11                                     -0.120734  \n",
       "12                                     -0.153314  \n",
       "13                                     -0.072756  \n",
       "14                                      0.120987  \n",
       "15                                      0.016452  \n",
       "16                                     -0.116972  \n",
       "17                                     -0.019712  \n",
       "18                                     -0.027544  \n",
       "19                                      0.165601  \n",
       "20                                     -0.074422  \n",
       "21                                      0.001517  \n",
       "22                                     -0.162969  \n",
       "23                                     -0.038755  \n",
       "24                                      0.077327  \n",
       "25                                      0.068165  \n",
       "26                                     -0.073575  \n",
       "27                                      0.077587  \n",
       "28                                     -0.127131  \n",
       "29                                      0.064757  \n",
       "30                                     -0.002818  \n",
       "31                                     -0.204979  \n",
       "32                                     -0.161267  \n",
       "33                                     -0.021777  \n",
       "34                                      0.847823  \n",
       "35                                      0.074611  \n",
       "36                                      0.265332  \n",
       "37                                      0.116973  \n",
       "38                                      0.198556  \n",
       "39                                      0.513159  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Model Coeficient Table\n",
    "Model_Coef_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb91ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e1c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare independent variables (X) and dependent variable (y)\n",
    "\n",
    "# Based on our prior analysis, I've decided to test the perfomance of the models with less predictors. \n",
    "# So will drop most of them and leave only the top 10.\n",
    "\n",
    "# To avoid writing them out every time, we save the names of the estimators of our model in a list. \n",
    "independent_variables=[#PIX\n",
    "            'DEEP_PIX_202205',\n",
    "            'DEEP_BILLS_202204',\n",
    "            'DEEP_BILLS_202205',\n",
    "            'DEEP_CARDS_202205',\n",
    "            'DEEP_CHECKING_202205',\n",
    "            'DEEP_PAYMENTS_202205',\n",
    "            'AMP_202202',\n",
    "            'AMP_202203',\n",
    "            'AMP_202204',\n",
    "            'AMP_202205'\n",
    "           ]\n",
    "\n",
    "X1 = data[independent_variables]\n",
    "y1 = data['FLG_202206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8931dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Baseline  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.38      0.17       423\n",
      "           1       0.98      0.90      0.94     13355\n",
      "\n",
      "    accuracy                           0.88     13778\n",
      "   macro avg       0.54      0.64      0.55     13778\n",
      "weighted avg       0.95      0.88      0.91     13778\n",
      "\n",
      "[[  162   261]\n",
      " [ 1337 12018]]\n",
      "\n",
      "Total processing time: --- 4.432544708251953 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Random Over Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.41      0.16       423\n",
      "           1       0.98      0.88      0.93     13355\n",
      "\n",
      "    accuracy                           0.87     13778\n",
      "   macro avg       0.54      0.65      0.54     13778\n",
      "weighted avg       0.95      0.87      0.90     13778\n",
      "\n",
      "[[  175   248]\n",
      " [ 1601 11754]]\n",
      "\n",
      "Total processing time: --- 10.018962621688843 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with SMOTE  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.48      0.16       423\n",
      "           1       0.98      0.86      0.92     13355\n",
      "\n",
      "    accuracy                           0.85     13778\n",
      "   macro avg       0.54      0.67      0.54     13778\n",
      "weighted avg       0.95      0.85      0.89     13778\n",
      "\n",
      "[[  204   219]\n",
      " [ 1887 11468]]\n",
      "\n",
      "Total processing time: --- 10.738734245300293 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Near Miss KNN  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.38      0.17       423\n",
      "           1       0.98      0.90      0.94     13355\n",
      "\n",
      "    accuracy                           0.88     13778\n",
      "   macro avg       0.54      0.64      0.55     13778\n",
      "weighted avg       0.95      0.88      0.91     13778\n",
      "\n",
      "[[  162   261]\n",
      " [ 1337 12018]]\n",
      "\n",
      "Total processing time: --- 4.903594017028809 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Random Forest with Random Under Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.75      0.14       423\n",
      "           1       0.99      0.73      0.84     13355\n",
      "\n",
      "    accuracy                           0.73     13778\n",
      "   macro avg       0.53      0.74      0.49     13778\n",
      "weighted avg       0.96      0.73      0.82     13778\n",
      "\n",
      "[[ 316  107]\n",
      " [3623 9732]]\n",
      "\n",
      "Total processing time: --- 0.6150894165039062 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Baseline  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.68      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.76      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 357   66]\n",
      " [4210 9145]]\n",
      "\n",
      "Total processing time: --- 0.15397953987121582 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Random Over Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.69      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.76      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 357   66]\n",
      " [4202 9153]]\n",
      "\n",
      "Total processing time: --- 0.32234716415405273 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with SMOTE  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.83      0.14       423\n",
      "           1       0.99      0.69      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.76      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 352   71]\n",
      " [4154 9201]]\n",
      "\n",
      "Total processing time: --- 0.43214869499206543 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Near Miss KNN  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.68      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.76      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 357   66]\n",
      " [4210 9145]]\n",
      "\n",
      "Total processing time: --- 0.1513807773590088 seconds ---\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Logistic Regression with Random Under Sampling  Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.84      0.14       423\n",
      "           1       0.99      0.68      0.81     13355\n",
      "\n",
      "    accuracy                           0.69     13778\n",
      "   macro avg       0.54      0.76      0.48     13778\n",
      "weighted avg       0.96      0.69      0.79     13778\n",
      "\n",
      "[[ 357   66]\n",
      " [4239 9116]]\n",
      "\n",
      "Total processing time: --- 0.09753775596618652 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#set shared variables\n",
    "random_state = 26\n",
    "test_size = 0.15\n",
    "verbose = 'off'\n",
    "#set model names\n",
    "models = ['Random Forest',\n",
    "          'Logistic Regression',\n",
    "         ]\n",
    "#set resampling names\n",
    "resamplers = [\n",
    "              'Baseline',\n",
    "              'Random Over Sampling',\n",
    "              'SMOTE',\n",
    "              'Near Miss KNN',\n",
    "              'Random Under Sampling',\n",
    "             ]\n",
    "\n",
    "#Instantiate empy models\n",
    "model_prediction_revisited = []\n",
    "\n",
    "for model_name in models:\n",
    "    for resampler_name in resamplers:\n",
    "        model,cr,cm,precision,recall,fbeta_score,support = u.model_predict(model_name,resampler_name,X1,y1,random_state,test_size,verbose)\n",
    "        model_prediction_revisited.append((model,cr,cm,precision,recall,fbeta_score,support,model_name,resampler_name))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa780658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "RandomForestClassifier(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26)\n",
      "LogisticRegression(class_weight='balanced', random_state=26)\n"
     ]
    }
   ],
   "source": [
    "# Create Model Coeficient Table\n",
    "Model_Coef_Table = u.Model_Coef_Table(model_prediction_revisited,X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e33c90",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Though the exploratory analysis indicated the possibily of finding correlation between transaction patterns and innactivity, the two classifiers and 4 resampling techniques used did not present good performance on this highly imbalanced dataset. \n",
    "\n",
    "The models just didn't perform well! Unfortunatelly. But hey, this is a scientific approach, know that something doesn't work is also a valid result, it just brushes off the false positives from your line of sight.\n",
    "\n",
    "All models had precision scores ranging from 0.08 to 0.09, recall from 0.83 to 0.85 and f1-score at exactlly 0.15. The main difference seeable at the confusion matrix, with slight differences on the true/false positive/negative predictions. The RandomForest with Random Under Sampling had similiar measures: precision at 0.09, recall at 0.77 and f1-score at 0.16.\n",
    "Exemple of Classification Report and Confusion Matrix for the Logistic Regression with Baseline model.\n",
    "\n",
    "The models actually did an interesting job of predicting 325+ cases of the 423 innactivity targets in the test set (you can see that looking at the confusion matrix's top left quadrant, 358 in the example above). That is why the Recall (or sensitivity) is high. \n",
    "\n",
    "This means the model is more confident at trying to predict the minority cases (the Random Forest Baseline practically didn't even try to predict the minority cases, in the report in only classified 15 as negatives, and 14 of them were flase - check the print screen bellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561716b5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Unfortunately this project doesn't seem to provide strong evidence towards answering either positively or negatively the business question provided.\n",
    "\n",
    "Our exploratory analysis show their is a potential correlation to be explored between innactivaty, depth (specially PIX) and amplitude. But, the use of classifier models, at least with the present configuration, haven't presented promising results.\n",
    "\n",
    "### Recommendations on future studies\n",
    "\n",
    "1. Study the use of time series prediction techniques as a subsititue for Classifiers\n",
    "2. Use the accumlative transactional variation on 5 months prior to the 6th month innactivity prediction may wielf better results than using the absolute number of transations per month as features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
